{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beto/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from transformers import logging as transformers_logging\n",
    "import tensorflow as tf\n",
    "from mongo_db_client import MongoDbClient\n",
    "from embedding_dataset import EmbeddingDataset\n",
    "import numpy as np\n",
    "from embedding_generator import EmbeddingGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Iterator, List, Literal, TypedDict, Any\n",
    "from embedding_generator import EmbeddingPairBatch, EmbeddingPair\n",
    "import random\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "random.seed(42)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CodeSearchNetLanguage = Literal['python', 'go', 'java', 'javascript', 'php', 'ruby']\n",
    "CodeSearchNetSplit = Literal['train', 'test', 'validation']\n",
    "\n",
    "class CodeSearchNetSample(TypedDict):\n",
    "  repository_name: str\n",
    "  func_path_in_repository: str\n",
    "  func_name: str\n",
    "  whole_func_string: str\n",
    "  language: CodeSearchNetLanguage\n",
    "  func_code_string: str\n",
    "  func_code_tokens: List[str]\n",
    "  func_documentation_string: str\n",
    "  func_documentation_string_tokens: List[str]\n",
    "  split_name: CodeSearchNetSplit\n",
    "  func_code_url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datasets import Dataset\n",
    "test_dataset = load_dataset('code_search_net', 'python', split='train', streaming=True).take(2).map(lambda _, index: { \"index\": index }, with_indices=True)\n",
    "[sample['index'] for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform IterableDataset to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(iterable_ds):\n",
    "  yield from iterable_ds\n",
    "\n",
    "ds = Dataset.from_generator(partial(gen, test_dataset), features=test_dataset.features)\n",
    "ds.save_to_disk('../datasets/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_ds = Dataset.load_from_disk(dataset_path='../datasets/test_dataset/')\n",
    "[sample['id'] for sample in loaded_ds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime and punish example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_ds = load_dataset('crime_and_punish', split='train[:1000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 228522.61 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "883"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "neg_samples_per_sample = 3\n",
    "dataset_len = 100\n",
    "indexes = np.random.randint(dataset_len-1, size=dataset_len)\n",
    "full_ds = []\n",
    "random_gen = default_rng(seed=42)\n",
    "\n",
    "def remove_empty_lines(sample) -> bool:\n",
    "  line: str = sample['line']\n",
    "  return len(line.split()) > 0\n",
    "\n",
    "for sample in crime_ds.filter(remove_empty_lines):\n",
    "  neg_indexes = random_gen.choice(indexes, neg_samples_per_sample, replace=False)\n",
    "  neg_samples = crime_ds.select(neg_indexes)\n",
    "  full_ds.append({\n",
    "    \"positive\": sample['line'],\n",
    "    \"negatives\": [neg_sample['line'] for neg_sample in neg_samples]\n",
    "  })\n",
    "\n",
    "len(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find equals\n",
    "eq_count = 0\n",
    "for sample in full_ds:\n",
    "  positive = sample['positive']\n",
    "  for neg in sample['negatives']:\n",
    "    if neg == positive:\n",
    "      eq_count += 1\n",
    "eq_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faiss index example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('crime_and_punish', split='train[:100]')\n",
    "next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_with_embeddings = ds.map(lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example[\"line\"], return_tensors=\"pt\"))[0][0].numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(ds_with_embeddings)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_with_embeddings.add_faiss_index(column='embeddings', faiss_verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "question = \"Is it serious ?\"\n",
    "question_embedding = q_encoder(**q_tokenizer(question, return_tensors=\"pt\"))[0][0].numpy()\n",
    "scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', question_embedding, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_with_embeddings.save_faiss_index('embeddings', '../datasets/faiss_index')\n",
    "ds_with_embeddings.drop_index('embeddings')\n",
    "ds_with_embeddings.save_to_disk('../datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dataset.load_from_disk('../datasets/indexes.faiss/')\n",
    "x.load_faiss_index('embeddings', '../datasets/faiss_index.faiss')\n",
    "next(iter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
