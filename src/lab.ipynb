{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beto/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from transformers import AutoTokenizer, TFAutoModel, logging as transformers_logging\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Literal\n",
    "from keras import Model, Sequential, backend as K\n",
    "from keras.layers import Dense, Input, Concatenate, Dropout\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.metrics import BinaryAccuracy, Precision, Recall\n",
    "from keras.optimizers import Adam\n",
    "from typing import List\n",
    "from mongo_db_client import MongoDbClient\n",
    "from models import MongoDbPairDoc\n",
    "import more_itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_max_length = 256\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = tf.cast(tf.tile(tf.expand_dims(attention_mask, -1), [1, 1, token_embeddings.shape[-1]]), tf.float32)\n",
    "    return tf.math.reduce_sum(token_embeddings * input_mask_expanded, 1) / tf.math.maximum(tf.math.reduce_sum(input_mask_expanded, 1), 1e-9)\n",
    "\n",
    "def generate_embeddings(sentences):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "    model = TFAutoModel.from_pretrained(\"bert-large-uncased\")\n",
    "    \n",
    "    encoded_input = tokenizer(\n",
    "        sentences, \n",
    "        padding='max_length', \n",
    "        max_length=embedding_max_length, \n",
    "        truncation=True, \n",
    "        return_tensors='tf',\n",
    "    )\n",
    "    model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_tokens(tokens) -> str:\n",
    "    parsed = ' '.join(tokens).replace('\\n', ' ')\n",
    "    parsed = ' '.join(parsed.strip().split())\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = MongoDbClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumDenseLayers = Literal[2, 4, 8]\n",
    "input_shape = (1024,) # TODO: Use variables\n",
    "hidden_layer_activation = 'relu'\n",
    "output_activation = 'sigmoid'\n",
    "dense_layers: Dict[NumDenseLayers, List] = {\n",
    "  2: [\n",
    "    Dense(100, activation=hidden_layer_activation),\n",
    "    Dense(50, activation=hidden_layer_activation),\n",
    "  ],\n",
    "  4: [\n",
    "    Dense(400, activation=hidden_layer_activation),\n",
    "    Dense(200, activation=hidden_layer_activation),\n",
    "    Dense(100, activation=hidden_layer_activation),\n",
    "    Dense(50, activation=hidden_layer_activation),\n",
    "  ], \n",
    "  8: [\n",
    "    Dense(800, activation=hidden_layer_activation),\n",
    "    Dense(600, activation=hidden_layer_activation),\n",
    "    Dense(500, activation=hidden_layer_activation),\n",
    "    Dense(400, activation=hidden_layer_activation),\n",
    "    Dense(300, activation=hidden_layer_activation),\n",
    "    Dense(200, activation=hidden_layer_activation),\n",
    "    Dense(100, activation=hidden_layer_activation),\n",
    "    Dense(50, activation=hidden_layer_activation),\n",
    "  ], \n",
    "}\n",
    "dropout_rate=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_hidden_layers: NumDenseLayers):\n",
    "  code_input = Input(\n",
    "    shape=input_shape,\n",
    "    name=\"code_embedding_input\",\n",
    "  )\n",
    "  comment_input = Input(\n",
    "    shape=input_shape,\n",
    "    name=\"comment_embedding_input\",\n",
    "  )\n",
    "  concatenated_inputs = Concatenate(axis=1)([code_input, comment_input])\n",
    "  dropout = Dropout(\n",
    "    dropout_rate,\n",
    "    name='dropout',\n",
    "  )(concatenated_inputs)\n",
    "  hidden_layers = Sequential(dense_layers[num_hidden_layers], name=\"hidden_layers\")(dropout)\n",
    "  output = Dense(1, activation=output_activation, name=\"output\")(hidden_layers)\n",
    "  model = Model(\n",
    "    inputs=[code_input, comment_input],\n",
    "    outputs=output,\n",
    "  )\n",
    "\n",
    "  threshold = 0.5\n",
    "  \n",
    "  model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss=BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "      BinaryAccuracy(\n",
    "        name=f\"Accurary (with threshold of {threshold})\", \n",
    "        threshold=threshold,\n",
    "      ),\n",
    "      Precision(thresholds=threshold),\n",
    "      Recall(thresholds=threshold),\n",
    "      # f1_score, # TODO: Reactivate\n",
    "    ],\n",
    "  )\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(pairs: List[MongoDbPairDoc]):\n",
    "  codes = [pre_process_tokens(pair['code_tokens']) for pair in pairs]\n",
    "  comments = [pre_process_tokens(pair['comment_tokens']) for pair in pairs]\n",
    "\n",
    "  return [generate_embeddings(codes), generate_embeddings(comments)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_model(8)\n",
    "num_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_targets(batch_size: int, dropout = 0.1):\n",
    "  array_shape = (batch_size)\n",
    "  ones_array = np.ones(array_shape)\n",
    "\n",
    "  random_mask = np.random.random(array_shape) < dropout\n",
    "  return ones_array * (1 - random_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pairs = list(db_client.get_pairs_collection().find({ \"partition\": \"train\", \"language\": \"python\" }).limit(batch_size))\n",
    "# model.fit(x=get_embeddings(train_pairs), y=get_random_targets(batch_size=batch_size), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_pairs = list(db_client.get_pairs_collection().find({ \"partition\": \"valid\", \"language\": \"python\" }).limit(batch_size))\n",
    "# model.predict(get_embeddings(valid_pairs), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an embedding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_count = 5000\n",
    "test_samples_count = 1000\n",
    "valid_samples_count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dataset_dir = '../datasets/embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '64aea2b37321b5c1dba81a3e'\n",
    "len(os.listdir(embedding_dataset_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_dataset(pairs: List[MongoDbPairDoc], batch_size = 100):\n",
    "  stored_pairs_ids = { pair_id.replace('.npy', ''): \"\" for pair_id in os.listdir(embedding_dataset_dir) if pair_id.endswith('.npy') }\n",
    "  \n",
    "  new_pairs = [pair for pair in pairs if str(pair['_id']) not in stored_pairs_ids]\n",
    "\n",
    "  for batch_pairs in more_itertools.chunked(new_pairs, batch_size):\n",
    "    [code_embeddings, comment_embeddings] = get_embeddings(batch_pairs)\n",
    "    for pair, code_embedding, comment_embedding in zip(batch_pairs, code_embeddings, comment_embeddings):\n",
    "      np.save(os.path.join(embedding_dataset_dir, f'{pair[\"_id\"]}.npy'), [code_embedding, comment_embedding])\n",
    "\n",
    "def get_stored_embeddings(pair_id: str):\n",
    "  return np.load(os.path.join(embedding_dataset_dir, f'{pair_id}.npy'))\n",
    "\n",
    "def validate_embeddings_dataset(pairs: List[MongoDbPairDoc]):\n",
    "  pairs_len = len(pairs)\n",
    "  if pairs_len > 100:\n",
    "    raise ValueError(\"The pairs length should be <= 100\")\n",
    "\n",
    "  random_index = random.randint(0, pairs_len - 1)\n",
    "  [code_embeddings, comment_embeddings] = get_embeddings(pairs)\n",
    "  [store_code_emb, store_comment_emb] = get_stored_embeddings(str(pairs[random_index][\"_id\"]))\n",
    "\n",
    "  correct_indexes = []\n",
    "  for index, (code_emb, comment_emb) in enumerate(zip(code_embeddings, comment_embeddings)):\n",
    "    is_correct = np.array_equal(code_emb, store_code_emb) and np.array_equal(comment_emb, store_comment_emb)\n",
    "    if is_correct:\n",
    "      correct_indexes.append(index)\n",
    "  \n",
    "  return len(correct_indexes) == 1 and correct_indexes[0] == random_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"train\", \"language\": \"python\" }).limit(train_samples_count)))\n",
    "save_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"test\", \"language\": \"python\" }).limit(test_samples_count)))\n",
    "save_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"valid\", \"language\": \"python\" }).limit(valid_samples_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is train dataset correct? True\n",
      "is test dataset correct? True\n",
      "is valid dataset correct? True\n"
     ]
    }
   ],
   "source": [
    "is_train_correct = validate_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"train\", \"language\": \"python\" }).limit(100)))\n",
    "is_test_correct = validate_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"test\", \"language\": \"python\" }).limit(100)))\n",
    "is_valid_correct = validate_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"valid\", \"language\": \"python\" }).limit(100)))\n",
    "\n",
    "print(f'is train dataset correct? {is_train_correct}') \n",
    "print(f'is test dataset correct? {is_test_correct}') \n",
    "print(f'is valid dataset correct? {is_valid_correct}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
