{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beto/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, TFAutoModel, logging as transformers_logging\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Literal, Tuple, List, Iterator\n",
    "from keras import Model, Sequential, callbacks\n",
    "from keras.layers import Dense, Input, Concatenate\n",
    "from keras.losses import Loss\n",
    "from keras.utils import losses_utils\n",
    "from keras.metrics import BinaryAccuracy, Precision, Recall\n",
    "from keras.optimizers import Adam\n",
    "from mongo_db_client import MongoDbClient\n",
    "from models import MongoDbPairDoc, Partition\n",
    "import more_itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_max_length = 256\n",
    "\n",
    "def pre_process_tokens(tokens) -> str:\n",
    "    parsed = ' '.join(tokens).replace('\\n', ' ')\n",
    "    parsed = ' '.join(parsed.strip().split())\n",
    "    return parsed\n",
    "\n",
    "# TODO: rename to 'generate_embeddings'\n",
    "def get_embeddings(pairs: List[MongoDbPairDoc]):\n",
    "  codes = [pre_process_tokens(pair['code_tokens']) for pair in pairs]\n",
    "  comments = [pre_process_tokens(pair['comment_tokens']) for pair in pairs]\n",
    "\n",
    "  return [generate_embeddings(codes), generate_embeddings(comments)]\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = tf.cast(tf.tile(tf.expand_dims(attention_mask, -1), [1, 1, token_embeddings.shape[-1]]), tf.float32)\n",
    "    return tf.math.reduce_sum(token_embeddings * input_mask_expanded, 1) / tf.math.maximum(tf.math.reduce_sum(input_mask_expanded, 1), 1e-9)\n",
    "\n",
    "# TODO: rename to generate_sentences_embeddings\n",
    "def generate_embeddings(sentences: List[str]):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "    model = TFAutoModel.from_pretrained(\"bert-large-uncased\")\n",
    "    \n",
    "    encoded_input = tokenizer(\n",
    "        sentences, \n",
    "        padding='max_length', \n",
    "        max_length=embedding_max_length, \n",
    "        truncation=True, \n",
    "        return_tensors='tf',\n",
    "    )\n",
    "    model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an embedding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_count = 5000\n",
    "test_samples_count = 1000\n",
    "valid_samples_count = 1000\n",
    "embedding_dataset_dir = '../datasets/embeddings/'\n",
    "db_client = MongoDbClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove unused functions\n",
    "def save_embeddings_dataset(pairs: List[MongoDbPairDoc], batch_size = 100):\n",
    "  stored_pairs_ids = { pair_id.replace('.npy', ''): \"\" for pair_id in os.listdir(embedding_dataset_dir) if pair_id.endswith('.npy') }\n",
    "  new_pairs = [pair for pair in pairs if str(pair['_id']) not in stored_pairs_ids]\n",
    "\n",
    "  with tqdm(total=len(new_pairs), desc=f\"Saving {len(new_pairs)} pairs into embedding dataset\") as progress_bar:\n",
    "    for batch_pairs in more_itertools.chunked(new_pairs, batch_size):\n",
    "      [code_embeddings, comment_embeddings] = get_embeddings(batch_pairs)\n",
    "      for pair, code_embedding, comment_embedding in zip(batch_pairs, code_embeddings, comment_embeddings):\n",
    "        np.save(os.path.join(embedding_dataset_dir, f'{pair[\"_id\"]}.npy'), [code_embedding, comment_embedding])\n",
    "      progress_bar.update(len(batch_pairs))\n",
    "\n",
    "def get_stored_embeddings(pair_id: str):\n",
    "  return np.load(os.path.join(embedding_dataset_dir, f'{pair_id}.npy'))\n",
    "\n",
    "def validate_embeddings_dataset(pairs: List[MongoDbPairDoc]):\n",
    "  pairs_len = len(pairs)\n",
    "  if pairs_len > 100:\n",
    "    raise ValueError(\"The pairs length should be <= 100\")\n",
    "\n",
    "  random_index = random.randint(0, pairs_len - 1)\n",
    "  [code_embeddings, comment_embeddings] = get_embeddings(pairs)\n",
    "  [store_code_emb, store_comment_emb] = get_stored_embeddings(str(pairs[random_index][\"_id\"]))\n",
    "\n",
    "  correct_indexes = []\n",
    "  for index, (code_emb, comment_emb) in enumerate(zip(code_embeddings, comment_embeddings)):\n",
    "    is_correct = np.array_equal(code_emb, store_code_emb) and np.array_equal(comment_emb, store_comment_emb)\n",
    "    if is_correct:\n",
    "      correct_indexes.append(index)\n",
    "  \n",
    "  return len(correct_indexes) == 1 and correct_indexes[0] == random_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(pairs: List[MongoDbPairDoc]) -> tf.data.Dataset:\n",
    "  def gen():\n",
    "    for pair in pairs:\n",
    "      pair_id = str(pair[\"_id\"])\n",
    "      [code_embedding, comment_embedding] = get_stored_embeddings(pair_id)\n",
    "      yield {\n",
    "        \"id\": pair_id,\n",
    "        \"code_embedding\": code_embedding,\n",
    "        \"comment_embedding\": comment_embedding,\n",
    "      }\n",
    "  \n",
    "  return tf.data.Dataset.from_generator(gen, output_types={ \n",
    "    \"id\": tf.string, \n",
    "    \"code_embedding\": tf.float32, \n",
    "    \"comment_embedding\": tf.float32,\n",
    "  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing embeddings pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"train\", \"language\": \"python\" }).limit(train_samples_count)))\n",
    "# save_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"test\", \"language\": \"python\" }).limit(test_samples_count)))\n",
    "# save_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"valid\", \"language\": \"python\" }).limit(valid_samples_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_train_correct = validate_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"train\", \"language\": \"python\" }).limit(100)))\n",
    "# is_test_correct = validate_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"test\", \"language\": \"python\" }).limit(100)))\n",
    "# is_valid_correct = validate_embeddings_dataset(list(db_client.get_pairs_collection().find({ \"partition\": \"valid\", \"language\": \"python\" }).limit(100)))\n",
    "\n",
    "# print(f'is train dataset correct? {is_train_correct}') \n",
    "# print(f'is test dataset correct? {is_test_correct}') \n",
    "# print(f'is valid dataset correct? {is_valid_correct}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumDenseLayers = Literal[2, 4, 8]\n",
    "input_shape = (1024,) # TODO: Use variables\n",
    "hidden_layer_activation = 'tanh'\n",
    "output_activation = 'sigmoid'\n",
    "dense_layers: Dict[NumDenseLayers, List] = {\n",
    "  2: [\n",
    "    Dense(100, activation=hidden_layer_activation),\n",
    "    Dense(50, activation=hidden_layer_activation),\n",
    "  ],\n",
    "  4: [\n",
    "    Dense(400, activation=hidden_layer_activation),\n",
    "    Dense(200, activation=hidden_layer_activation),\n",
    "    Dense(100, activation=hidden_layer_activation),\n",
    "    Dense(50, activation=hidden_layer_activation),\n",
    "  ], \n",
    "  8: [\n",
    "    Dense(800, activation=hidden_layer_activation),\n",
    "    Dense(600, activation=hidden_layer_activation),\n",
    "    Dense(500, activation=hidden_layer_activation),\n",
    "    Dense(400, activation=hidden_layer_activation),\n",
    "    Dense(300, activation=hidden_layer_activation),\n",
    "    Dense(200, activation=hidden_layer_activation),\n",
    "    Dense(100, activation=hidden_layer_activation),\n",
    "    Dense(50, activation=hidden_layer_activation),\n",
    "  ], \n",
    "}\n",
    "dropout_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrastiveLoss(Loss):\n",
    "   def __init__(self, reduction=losses_utils.ReductionV2.AUTO, name=\"constrastive_loss\", margin=1):\n",
    "      self.margin = margin\n",
    "      super().__init__(reduction, name)\n",
    "\n",
    "   def call(self, y_true, y_pred):\n",
    "      square_pred = tf.math.square(y_pred)\n",
    "      margin_square = tf.math.square(tf.math.maximum(self.margin - (y_pred), 0))\n",
    "      return tf.math.reduce_mean(\n",
    "        (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "      )\n",
    "\n",
    "def build_model(num_hidden_layers: NumDenseLayers):\n",
    "  code_input = Input(\n",
    "    shape=input_shape,\n",
    "    name=\"code_embedding_input\",\n",
    "  )\n",
    "  comment_input = Input(\n",
    "    shape=input_shape,\n",
    "    name=\"comment_embedding_input\",\n",
    "  )\n",
    "  concatenated_inputs = Concatenate(axis=1)([code_input, comment_input])\n",
    "  hidden_layers = Sequential(dense_layers[num_hidden_layers], name=\"hidden_layers\")(concatenated_inputs)\n",
    "  output = Dense(1, activation=output_activation, name=\"output\")(hidden_layers)\n",
    "  model = Model(\n",
    "    inputs=[code_input, comment_input],\n",
    "    outputs=output,\n",
    "    name=\"embedding_comparator\"\n",
    "  )\n",
    "\n",
    "  model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss=ConstrastiveLoss(),\n",
    "    metrics=[\n",
    "      BinaryAccuracy(),\n",
    "      Precision(name=\"precision\"),\n",
    "      Recall(name=\"recall\"),\n",
    "      # f1_score, # TODO: Reactivate\n",
    "    ],\n",
    "  )\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_pairs(pairs: List[MongoDbPairDoc]):\n",
    "  negative_pairs = pairs.copy()\n",
    "  random.shuffle(negative_pairs)\n",
    "  return negative_pairs\n",
    "\n",
    "def generate_dataset(samples_count = 1000, batch_size=100, partition: Partition = 'train') -> Iterator[Tuple[List[np.ndarray], np.ndarray]]:\n",
    "  def get_targets_array(size: int):\n",
    "    targets = np.empty((size, ))\n",
    "    targets[::2] = 1\n",
    "    targets[1::2] = 0\n",
    "    return targets\n",
    "\n",
    "  pairs = list(db_client.get_pairs_collection().find({ \"language\": \"python\", \"partition\": partition }).limit(samples_count))\n",
    "  \n",
    "  for batch_pairs in more_itertools.chunked(pairs, batch_size):\n",
    "    if len(batch_pairs) != batch_size:\n",
    "      continue\n",
    "\n",
    "    negative_pairs = generate_negative_pairs(batch_pairs)\n",
    "    code_embedings, comment_embeddings = [], []\n",
    "\n",
    "    for pair, negative_pair in zip(batch_pairs, negative_pairs):\n",
    "      code_emb, comment_emb = get_stored_embeddings(str(pair['_id']))\n",
    "      negative_code_emb, negative_comment_emb = get_stored_embeddings(str(negative_pair['_id']))\n",
    "\n",
    "      code_embedings.append(code_emb)\n",
    "      comment_embeddings.append(comment_emb)\n",
    "      \n",
    "      code_embedings.append(code_emb)\n",
    "      comment_embeddings.append(negative_comment_emb)\n",
    "    \n",
    "    stacked_code_embeddings = np.stack(code_embedings)\n",
    "    stacked_comment_embeddings = np.stack(comment_embeddings)\n",
    "    yield [stacked_code_embeddings, stacked_comment_embeddings], get_targets_array(stacked_code_embeddings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_comparator = build_model(num_hidden_layers=2)\n",
    "embedding_comparator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board_callback = callbacks.TensorBoard(log_dir=f\"../logs/scalars/{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "for inputs, target in generate_dataset(samples_count=train_samples_count):\n",
    "    embedding_comparator.fit(\n",
    "        inputs, \n",
    "        target,\n",
    "        shuffle=True,\n",
    "        epochs=10,\n",
    "        batch_size=inputs[0].shape[0],\n",
    "        callbacks=[tensor_board_callback],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
