{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from typing import TypedDict, Literal, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CodeSearchNetLanguage = Literal['python', 'go', 'java', 'javascript', 'php', 'ruby']\n",
    "CodeSearchNetSplit = Literal['train', 'test', 'validation']\n",
    "\n",
    "class CodeSearchNetSample(TypedDict):\n",
    "  repository_name: str\n",
    "  func_path_in_repository: str\n",
    "  func_name: str\n",
    "  whole_func_string: str\n",
    "  language: CodeSearchNetLanguage\n",
    "  func_code_string: str\n",
    "  func_code_tokens: List[str]\n",
    "  func_documentation_string: str\n",
    "  func_documentation_string_tokens: List[str]\n",
    "  split_name: CodeSearchNetSplit\n",
    "  func_code_url: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process and save pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "\n",
    "dataset_name = \"code_search_net\"\n",
    "\n",
    "def load(language: CodeSearchNetLanguage, split: CodeSearchNetSplit, take: int) -> Dataset:\n",
    "  ds = cast(Dataset, load_dataset(dataset_name, language, split=split))\n",
    "  return Dataset.from_dict(ds[:take])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "random_generator = default_rng(seed=42)\n",
    "batch_size = 100\n",
    "\n",
    "def generate_negative_samples(iterator: Iterator, negative_samples_per_sample: int):\n",
    "  for batched_sample in iterator:\n",
    "    processed_codes = [' '.join(code_tokens) for code_tokens in batched_sample['func_code_tokens']]\n",
    "    processed_comments = [' '.join(comment_tokens) for comment_tokens in batched_sample['func_documentation_tokens']]\n",
    "    batch_indexes = range(len(processed_codes))\n",
    "\n",
    "    for index, code, comment in zip(batch_indexes, processed_codes, processed_comments):\n",
    "      indexes = [i for i in batch_indexes if i != index]\n",
    "      negative_indexes = random_generator.choice(indexes, negative_samples_per_sample, replace=False)\n",
    "      negative_comments = np.array(processed_comments)[negative_indexes]\n",
    "      for negative_comment in negative_comments:\n",
    "        yield {\n",
    "          \"code\": code,\n",
    "          \"comment_positive\": comment,\n",
    "          \"comment_negative\": negative_comment,\n",
    "        }\n",
    "\n",
    "def pre_process_sample(sample):\n",
    "  return {\n",
    "    \"code\": ' '.join(sample['func_code_tokens']),\n",
    "    \"comment\": ' '.join(sample['func_documentation_tokens'])\n",
    "  }\n",
    "\n",
    "train_ds: Dataset = load('python', 'train', take=1000).map(pre_process_sample)\n",
    "\n",
    "full_ds: Dataset = Dataset.from_generator(lambda: generate_negative_samples(train_ds.iter(batch_size=batch_size), 3)) # type: ignore\n",
    "\n",
    "full_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "processed_comments = [' '.join(sample['func_documentation_tokens']) for sample in load('python', 'train', 100)] # type: ignore\n",
    "tokenizer.fit_on_sequences(processed_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "max_sequence_length = 100\n",
    "\n",
    "\n",
    "def model_test():\n",
    "  # Define input layers\n",
    "  input_layer1 = Input(shape=(max_sequence_length,), name='code')\n",
    "  input_layer2 = Input(shape=(max_sequence_length,), name='comment_positive')\n",
    "  input_layer3 = Input(shape=(max_sequence_length,), name='comment_negative')\n",
    "\n",
    "  print(input_layer1)\n",
    "  print(input_layer2)\n",
    "  print(input_layer3)\n",
    "\n",
    "  # Embedding layers\n",
    "  embedding_dim = 50\n",
    "  embedding_layer1 = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(input_layer1)\n",
    "  embedding_layer2 = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(input_layer2)\n",
    "  embedding_layer3 = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(input_layer3)\n",
    "\n",
    "  # Flatten the embedding layers\n",
    "  flatten_layer1 = Flatten()(embedding_layer1)\n",
    "  flatten_layer2 = Flatten()(embedding_layer2)\n",
    "  flatten_layer3 = Flatten()(embedding_layer3)\n",
    "\n",
    "  # Concatenate the flattened embeddings\n",
    "  concatenated = Concatenate()([flatten_layer1, flatten_layer2, flatten_layer3])\n",
    "\n",
    "  # Dense layer for classification\n",
    "  dense_layer = Dense(64, activation='relu')(concatenated)\n",
    "\n",
    "  # Output layer\n",
    "  output_layer = Dense(1, activation='sigmoid', name='output')(dense_layer)\n",
    "\n",
    "  # Create the model\n",
    "  model = Model(inputs=[input_layer1, input_layer2, input_layer3], outputs=output_layer)\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_dataset = full_ds.to_tf_dataset(batch_size=batch_size)\n",
    "\n",
    "model.fit(train_tf_dataset, epochs=10, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
