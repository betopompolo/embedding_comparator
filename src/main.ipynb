{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requirements\n",
    "Make sure you're using the local conda env for running this notebook. If is not created yet, create one with python 3.9 by running `conda create --name myenv python=3.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (23.3.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 1)) (4.34.1)\n",
      "Requirement already satisfied: datasets in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 2)) (2.14.5)\n",
      "Requirement already satisfied: numpy in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 3)) (1.26.1)\n",
      "Requirement already satisfied: pandas in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: tensorflow in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 5)) (2.14.0)\n",
      "Requirement already satisfied: tensorflow-macos in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 6)) (2.14.0)\n",
      "Requirement already satisfied: tensorflow-metal in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: sentence-transformers in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 8)) (2.2.2)\n",
      "Requirement already satisfied: plotly==5.18.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 9)) (5.18.0)\n",
      "Requirement already satisfied: nbformat in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 10)) (5.9.2)\n",
      "Requirement already satisfied: kaleido in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 11)) (0.2.1)\n",
      "Requirement already satisfied: more-itertools in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 12)) (10.1.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from plotly==5.18.0->-r ../requirements.txt (line 9)) (8.2.3)\n",
      "Requirement already satisfied: packaging in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from plotly==5.18.0->-r ../requirements.txt (line 9)) (23.2)\n",
      "Requirement already satisfied: filelock in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from transformers->-r ../requirements.txt (line 1)) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from transformers->-r ../requirements.txt (line 1)) (0.17.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from transformers->-r ../requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from transformers->-r ../requirements.txt (line 1)) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from transformers->-r ../requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from transformers->-r ../requirements.txt (line 1)) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from transformers->-r ../requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from transformers->-r ../requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from datasets->-r ../requirements.txt (line 2)) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from datasets->-r ../requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from datasets->-r ../requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from datasets->-r ../requirements.txt (line 2)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets->-r ../requirements.txt (line 2)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from datasets->-r ../requirements.txt (line 2)) (3.8.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from pandas->-r ../requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from pandas->-r ../requirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from pandas->-r ../requirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (4.24.4)\n",
      "Requirement already satisfied: setuptools in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-macos->-r ../requirements.txt (line 6)) (2.14.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorflow-metal->-r ../requirements.txt (line 7)) (0.41.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from sentence-transformers->-r ../requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from sentence-transformers->-r ../requirements.txt (line 8)) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from sentence-transformers->-r ../requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: scipy in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from sentence-transformers->-r ../requirements.txt (line 8)) (1.11.3)\n",
      "Requirement already satisfied: nltk in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from sentence-transformers->-r ../requirements.txt (line 8)) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from sentence-transformers->-r ../requirements.txt (line 8)) (0.1.99)\n",
      "Requirement already satisfied: fastjsonschema in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from nbformat->-r ../requirements.txt (line 10)) (2.18.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from nbformat->-r ../requirements.txt (line 10)) (4.19.1)\n",
      "Requirement already satisfied: jupyter-core in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from nbformat->-r ../requirements.txt (line 10)) (5.3.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from nbformat->-r ../requirements.txt (line 10)) (5.11.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ../requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ../requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ../requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ../requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ../requirements.txt (line 2)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ../requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from aiohttp->datasets->-r ../requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->-r ../requirements.txt (line 10)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->-r ../requirements.txt (line 10)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat->-r ../requirements.txt (line 10)) (0.10.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from requests->transformers->-r ../requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from requests->transformers->-r ../requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from requests->transformers->-r ../requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: sympy in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers->-r ../requirements.txt (line 8)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers->-r ../requirements.txt (line 8)) (3.2)\n",
      "Requirement already satisfied: jinja2 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers->-r ../requirements.txt (line 8)) (3.1.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from jupyter-core->nbformat->-r ../requirements.txt (line 10)) (3.11.0)\n",
      "Requirement already satisfied: click in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from nltk->sentence-transformers->-r ../requirements.txt (line 8)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from nltk->sentence-transformers->-r ../requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from scikit-learn->sentence-transformers->-r ../requirements.txt (line 8)) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from torchvision->sentence-transformers->-r ../requirements.txt (line 8)) (10.1.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (6.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers->-r ../requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos->-r ../requirements.txt (line 6)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install -r \"../requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beto/Projects/embedding_comparator/.conda/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets\n",
    "dataset_name = \"code_search_net\"\n",
    "\n",
    "def load_from_cs_net(take: int) -> Dataset:\n",
    "  ds = load_dataset(dataset_name, 'python', split='train')\n",
    "  return Dataset.from_dict(ds[:take]) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "comment_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "code_model = SentenceTransformer('flax-sentence-embeddings/st-codesearch-distilroberta-base')\n",
    "embedding_shape = (768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "random_generator = default_rng(seed=42)\n",
    "\n",
    "def generate_negative_samples(iterator: Iterator, negative_samples_per_sample: int):\n",
    "  for batched_sample in iterator:\n",
    "    codes_embeddings = batched_sample['code_embedding']\n",
    "    comments_embeddings = batched_sample['comment_embedding']\n",
    "    batch_indexes = range(len(codes_embeddings))\n",
    "\n",
    "    for index in batch_indexes:\n",
    "      indexes = [i for i in batch_indexes if i != index]\n",
    "      negative_indexes = random_generator.choice(indexes, negative_samples_per_sample, replace=False)\n",
    "\n",
    "      yield {\n",
    "        \"code_embedding\": codes_embeddings[index],\n",
    "        \"comment_embedding\": comments_embeddings[index],\n",
    "        \"target\": 1\n",
    "      }\n",
    "\n",
    "      for negative_index in negative_indexes:\n",
    "        yield {\n",
    "          \"code_embedding\": codes_embeddings[index],\n",
    "          \"comment_embedding\": comments_embeddings[negative_index],\n",
    "          \"target\": 0\n",
    "        }\n",
    "\n",
    "def with_neg_samples(dataset: Dataset, negative_samples_per_sample: int, batch_size = 100) -> Dataset:\n",
    "  assert negative_samples_per_sample <= batch_size, \"negative_samples_per_sample must not be greater than batch_size\"\n",
    "  if negative_samples_per_sample <= 0:\n",
    "    return dataset\n",
    "  \n",
    "  dataset_with_negative_samples: Dataset = Dataset.from_generator(lambda: generate_negative_samples(dataset.iter(batch_size=batch_size), negative_samples_per_sample)) # type: ignore\n",
    "  return dataset_with_negative_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embedding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "train_count = 2000\n",
    "train_dataset_path = f'../datasets/embeddings_python_train_{train_count}'\n",
    "train_pairs = load_from_cs_net(train_count)\n",
    "is_embeddings_dataset_stored = os.path.isdir(train_dataset_path)\n",
    "\n",
    "def generate_embeddings_in_batch(batched_sample):\n",
    "  codes = batched_sample['func_code_string']\n",
    "  comments = batched_sample['func_documentation_string']\n",
    "\n",
    "  return {\n",
    "    \"code_embedding\": code_model.encode(codes),\n",
    "    \"comment_embedding\": comment_model.encode(comments),\n",
    "  }\n",
    "\n",
    "embeddings_dataset: Dataset = Dataset.from_dict(load_from_disk(train_dataset_path)[:train_count]) if is_embeddings_dataset_stored else train_pairs.map(\n",
    "  generate_embeddings_in_batch, \n",
    "  batched=True, \n",
    "  batch_size=100,\n",
    "  remove_columns=list(train_pairs[0].keys()),\n",
    "  desc=\"Generating embeddings\"\n",
    ") # type: ignore\n",
    "\n",
    "if is_embeddings_dataset_stored == False:\n",
    "  embeddings_dataset.save_to_disk(train_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf_fit_dataset(negative_samples_per_sample: int):\n",
    "  tf_dataset = with_neg_samples(embeddings_dataset.shuffle(), negative_samples_per_sample).to_tf_dataset().map(lambda sample: ({\n",
    "    \"code_embedding\": sample[\"code_embedding\"],\n",
    "    \"comment_embedding\": sample[\"comment_embedding\"],\n",
    "  }, sample[\"target\"]))\n",
    "  dataset_size = tf_dataset.cardinality().numpy() # type: ignore\n",
    "  print(f\"training with {dataset_size} samples\")\n",
    "  validation_samples_count = int(dataset_size * 0.2)\n",
    "  \n",
    "  validation_ds = tf_dataset.take(validation_samples_count)\n",
    "  train_ds = tf_dataset.skip(validation_samples_count)\n",
    "\n",
    "  return (train_ds, validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "from models import build_dense_model\n",
    "\n",
    "\n",
    "neg_samples_count = [5]\n",
    "num_hidden_layers = 4\n",
    "for neg_count in neg_samples_count:\n",
    "  model = build_dense_model(num_hidden_layers=num_hidden_layers, input_shape=embedding_shape, model_name=f'dense_{num_hidden_layers}_neg_{neg_count}-dropout20')\n",
    "  train, validation = to_tf_fit_dataset(neg_count)\n",
    "  tensor_board_callback = callbacks.TensorBoard(log_dir=f'../logs/{model.name}')\n",
    "\n",
    "  model.fit(\n",
    "    train.batch(batch_size),\n",
    "    validation_data=validation.batch(batch_size),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch,\n",
    "    callbacks=[tensor_board_callback]\n",
    "  )\n",
    "  model.save(f'../models/{model.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, TypedDict, List\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "class SimilarityInput(TypedDict):\n",
    "  code_embedding: Any\n",
    "  comment_embedding: Any\n",
    "\n",
    "class SimilaryResult(TypedDict):\n",
    "  similarity: np.float32\n",
    "  pair_index: int\n",
    "\n",
    "class SearchResult(TypedDict):\n",
    "  similarity: np.float32\n",
    "  pair_index: int\n",
    "  query: str\n",
    "\n",
    "\n",
    "def get_similarities(inputs: List[SimilarityInput], model) -> List[SimilaryResult]:\n",
    "  tf_inputs = Dataset.from_list(inputs).to_tf_dataset(batch_size=10)\n",
    "  similarities = model.predict(tf_inputs, verbose=0).flatten()\n",
    "  results = [{ \"similarity\": similarity, \"pair_index\": index } for index, similarity in enumerate(similarities)]\n",
    "  return results # type: ignore\n",
    "\n",
    "\n",
    "def search(query, model) -> List[SearchResult]:\n",
    "  query_embedding = comment_model.encode([query]).flatten()\n",
    "  similarities = list(get_similarities(\n",
    "    inputs=[ { \"code_embedding\": embedding_pair['code_embedding'], \"comment_embedding\": query_embedding } for embedding_pair in embeddings_dataset],\n",
    "    model=model\n",
    "  ))\n",
    "\n",
    "  return [{\n",
    "    \"similarity\": similarity['similarity'],\n",
    "    \"pair_index\": similarity['pair_index'],\n",
    "    \"query\": query\n",
    "  } for similarity in similarities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentQuery(TypedDict):\n",
    "  comment_query: str\n",
    "  removed_word: str\n",
    "\n",
    "def generate_comment_queries(comment_tokens: List[str], max_words_to_remove = 30) -> Iterator[CommentQuery]:\n",
    "  words_to_remove_count = min(len(comment_tokens), max_words_to_remove)\n",
    "  word_indexes_to_remove = list(range(words_to_remove_count))\n",
    "  random_generator.shuffle(word_indexes_to_remove)\n",
    "\n",
    "  for word_index in word_indexes_to_remove:\n",
    "    comment_tokens_copy = comment_tokens.copy()\n",
    "    removed_word = comment_tokens_copy.pop(word_index)\n",
    "    comment_query = ' '.join(comment_tokens_copy)\n",
    "    yield {\n",
    "      \"comment_query\": comment_query,\n",
    "      \"removed_word\": removed_word\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import more_itertools\n",
    "\n",
    "def wrap(value: str, max_width = 20) -> str:\n",
    "  return \"<br>\".join([' '.join(sentence) for sentence in more_itertools.chunked(value.split(' '), n=max_width)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "class ExperimentLog:\n",
    "  def __init__(self, experiment_name: str) -> None:\n",
    "    self.experiment_name = experiment_name\n",
    "    self.experiment_results_dir = f'../results/{self.experiment_name}'\n",
    "    if not os.path.isdir(self.experiment_results_dir):\n",
    "      os.makedirs(self.experiment_results_dir)\n",
    "  \n",
    "  def save_figure(self, figure: go.Figure, figure_name: str):\n",
    "    image_extension = '.png'\n",
    "    figure_name_path = figure_name if figure_name.endswith(image_extension) else f'{figure_name}{image_extension}'\n",
    "    figure.write_image(os.path.join(self.experiment_results_dir, figure_name_path))\n",
    "\n",
    "  def save_file(self, data: str, file_name: str):\n",
    "    text_extension = '.txt'\n",
    "    file_name_path = file_name if file_name.endswith(text_extension) else f'{file_name}{text_extension}'\n",
    "    with open(os.path.join(self.experiment_results_dir, file_name_path), 'w') as file:\n",
    "      file.write(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generalization with a single comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generalization_experiment(search_model, samples_count):\n",
    "  for sample_index in range(samples_count):\n",
    "    comment_tokens: list = train_pairs[sample_index]['func_documentation_tokens']\n",
    "    experiment_results = []\n",
    "    \n",
    "    for index, comment_query in enumerate(generate_comment_queries(comment_tokens)) :\n",
    "      similarity = get_similarity(\n",
    "        query=comment_query['comment_query'],\n",
    "        code_embedding=embeddings_dataset[sample_index]['code_embedding'],\n",
    "        model=search_model,\n",
    "      )\n",
    "\n",
    "      experiment_results.append({\n",
    "        \"removed_word\": comment_query['removed_word'],\n",
    "        \"similarity\": similarity,\n",
    "        \"original_query\": ' '.join(comment_tokens),\n",
    "        \"index\": index,\n",
    "      })\n",
    "\n",
    "    yield experiment_results\n",
    "\n",
    "def get_similarity(query, code_embedding, model) -> np.float32:\n",
    "  query_embedding = comment_model.encode([query]).flatten()\n",
    "  predictions = get_similarities(\n",
    "    inputs=[{ \"code_embedding\": code_embedding, 'comment_embedding': query_embedding }], \n",
    "    model=model\n",
    "  )\n",
    "  return predictions[0]['similarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the experiment and plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_model = load_model('../models/dense_4_neg_5/')\n",
    "experiment_1_log = ExperimentLog('experiment_1')\n",
    "bad_sample_threshold = 0.2\n",
    "\n",
    "for index, results in enumerate(run_generalization_experiment(search_model=search_model, samples_count=100)):\n",
    "  sorted_results = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
    "  n_samples = len(sorted_results)\n",
    "\n",
    "  data_frame = pd.DataFrame({\n",
    "    \"removed word\": [result['removed_word'] for result in sorted_results],\n",
    "    \"similarity\": [result['similarity'] for result in sorted_results],\n",
    "  })\n",
    "\n",
    "  original_query = wrap(sorted_results[0]['original_query'], 15)\n",
    "  fig = px.bar(data_frame, x=data_frame.index, y=\"similarity\", text_auto=True, title=f\"<sup>Original query: {original_query}</sup>\")\n",
    "  fig.update_xaxes(title_text='removed word', tickvals=data_frame.index, ticktext=data_frame[\"removed word\"].tolist())\n",
    "  experiment_1_log.save_figure(fig, f'sample_{index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generalization with comments + search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_match_index(search_ranking: List[SearchResult], sample_index: int):\n",
    "  for ranking_index, search_result in enumerate(search_ranking):\n",
    "    if search_result['pair_index'] == sample_index:\n",
    "      return ranking_index\n",
    "  raise ValueError(\"Pair not found in search ranking\")\n",
    "\n",
    "\n",
    "def run_experiment_2(search_model, pairs_count: int):\n",
    "  for sample_index in tqdm(range(pairs_count), total=pairs_count, desc=\"Running experiment 2\"):\n",
    "    experiment_results = []\n",
    "    pair = train_pairs[sample_index]\n",
    "    original_query = ' '.join(pair[\"func_documentation_tokens\"])\n",
    "\n",
    "    for comment_query in generate_comment_queries(pair['func_documentation_tokens']):\n",
    "      ranking = search(query=comment_query, model=search_model)\n",
    "      similarity_rank = sorted(ranking, key=lambda it: float(it['similarity']), reverse=True)\n",
    "\n",
    "      experiment_results.append({\n",
    "        \"sample_index\": sample_index,\n",
    "        \"search_match_index\": get_search_match_index(similarity_rank, sample_index),\n",
    "        \"original_query\": original_query,\n",
    "        \"removed_word\": comment_query[\"removed_word\"]\n",
    "      })\n",
    "      \n",
    "    yield experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_rate(results, k: int) -> float:\n",
    "  results_count = len(results)\n",
    "\n",
    "  hits = sum([result['search_match_index'] < k for result in results])\n",
    "  success_rate_k = 0 if hits == 0 else hits / results_count\n",
    "  return success_rate_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "Running experiment 2: 100%|██████████| 100/100 [27:55<00:00, 16.75s/it]\n"
     ]
    }
   ],
   "source": [
    "search_model = load_model('../models/dense_4_neg_5/')\n",
    "experiment_2_log = ExperimentLog(experiment_name='experiment_2')\n",
    "experiment_2_results = list(run_experiment_2(search_model=search_model, pairs_count=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1439"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_total = 0\n",
    "for result in experiment_2_results:\n",
    "  samples_total += len(result)\n",
    "samples_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_values = [3, 5, 10]\n",
    "for k in k_values:\n",
    "  success_rates = [success_rate(results, k) for results in experiment_2_results]\n",
    "  sorted_success_rates = sorted(success_rates)\n",
    "  fig = px.line(pd.DataFrame(data={\n",
    "    \"Sample nº\": range(len(sorted_success_rates)),\n",
    "    \"Success rate\": sorted_success_rates,\n",
    "  }), x=\"Sample nº\", y=\"Success rate\", title=f\"Success rate {k}\")\n",
    "\n",
    "  experiment_2_log.save_figure(fig, f'experiment_2_success_rate_{k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generalization with CodeSearchNet queries + search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dict lookup: 100%|██████████| 457461/457461 [00:45<00:00, 9964.04it/s] \n"
     ]
    }
   ],
   "source": [
    "python_splits = load_dataset(dataset_name, 'python', split=['train', 'test', 'validation']) # type: ignore\n",
    "python_full_dataset = concatenate_datasets(python_splits) # type: ignore\n",
    "splits_info = python_splits[0].info.splits # type: ignore\n",
    "python_full_dataset_count = sum([splits_info[key].num_examples for key in splits_info.keys()]) # type: ignore\n",
    "\n",
    "pairs_dataset_lookup = { sample['func_code_url']: index  for index, sample in tqdm(enumerate(python_full_dataset), desc=\"Generating dict lookup\", total=python_full_dataset_count) }\n",
    "def search_by_url(url: str) -> Optional[int]:\n",
    "  try:\n",
    "    return pairs_dataset_lookup[url]\n",
    "  except:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_samples_path = '../datasets/query_samples'\n",
    "\n",
    "def remove_duplicates(dataset: Dataset) -> Dataset:\n",
    "  pandas_dataset = dataset.to_pandas().drop_duplicates(subset=['Language', 'Query', 'GitHubUrl', 'Relevance'], ignore_index=True) # type: ignore\n",
    "  dedup_dataset = Dataset.from_pandas(pandas_dataset)\n",
    "  return dedup_dataset\n",
    "\n",
    "def remove_queries_without_code(dataset: Dataset) -> Dataset:\n",
    "  return dataset.filter(lambda sample: search_by_url(sample['GitHubUrl']) is not None, desc=\"Filtering queries with no corresponding code\")\n",
    "\n",
    "def pre_process_query_samples() -> Dataset:\n",
    "  cs_net_queries_dataset: Dataset = Dataset.from_csv('../datasets/code_search_net_queries.csv') # type: ignore\n",
    "  \n",
    "  return remove_queries_without_code(remove_duplicates(cs_net_queries_dataset))\n",
    "\n",
    "def get_query_samples() -> Dataset:\n",
    "  try:\n",
    "    return Dataset.load_from_disk(query_samples_path)\n",
    "  except:\n",
    "    query_samples = pre_process_query_samples()\n",
    "    query_samples.save_to_disk(query_samples_path)\n",
    "    return query_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_code_embeddings(samples) -> Dataset:\n",
    "  query_texts = [sample['Query'] for sample in samples]\n",
    "  query_codes = [python_full_dataset[search_by_url(sample['GitHubUrl'])]['func_code_string'] for sample in samples]\n",
    "  assert len(query_texts) == len(query_codes), \"query_texts and query_codes arrays doesn't have the same length\"\n",
    "\n",
    "  query_embeddings = comment_model.encode(query_texts)\n",
    "  code_embeddings = code_model.encode(query_codes)\n",
    "\n",
    "  validation_dataset = []\n",
    "  for query_embedding, code_embedding in zip(query_embeddings, code_embeddings):\n",
    "    validation_dataset.append({\n",
    "      \"code_embedding\": code_embedding,\n",
    "      \"comment_embedding\": query_embedding,\n",
    "    })\n",
    "\n",
    "  return Dataset.from_list(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def validate(model, samples):\n",
    "  validation_dataset = get_query_code_embeddings(samples).to_tf_dataset(batch_size=10)\n",
    "\n",
    "  return {\n",
    "    \"predictions\": model.predict(validation_dataset, verbose=0).flatten(),\n",
    "    \"targets\": [sample['Relevance'] for sample in samples]\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prediction_correct(prediction, target) -> bool:\n",
    "  if target in [0, 1]:\n",
    "    return prediction <= 0.5\n",
    "  \n",
    "  if target in [2, 3]:\n",
    "    return prediction > 0.5\n",
    "  \n",
    "  raise ValueError(f\"target should be in range of [0, 3]. Instead, it has value of {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RestoredOptimizer` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RestoredOptimizer`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842\n"
     ]
    }
   ],
   "source": [
    "validation_query_samples = [sample for sample in get_query_samples() if sample['Language'].lower() == 'python']\n",
    "validation_query_samples_count = len(validation_query_samples)\n",
    "\n",
    "model = load_model('../models/dense_4_neg_5')\n",
    "search_result = validate(model, validation_query_samples)\n",
    "\n",
    "hits = sum([is_prediction_correct(prediction, target) for prediction, target in zip(search_result['predictions'], search_result['targets'])])\n",
    "success_percentage = hits / validation_query_samples_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_3_log = ExperimentLog('experiment_3')\n",
    "experiment_3_log.save_file(f'{hits} correct predictions out of {validation_query_samples_count} samples - {success_percentage:.2%} success rate', \"cs_net_queries_result.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
