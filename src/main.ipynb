{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requirements\n",
    "Make sure you're using the local conda env for running this notebook. If is not created yet, create one with python 3.9 by running `conda create --name myenv python=3.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r \"../requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets\n",
    "dataset_name = \"code_search_net\"\n",
    "\n",
    "def load_from_cs_net(take: int) -> Dataset:\n",
    "  ds = load_dataset(dataset_name, 'python', split='train')\n",
    "  return Dataset.from_dict(ds[:take]) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "comment_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "code_model = SentenceTransformer('flax-sentence-embeddings/st-codesearch-distilroberta-base')\n",
    "embedding_shape = (768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "random_generator = default_rng(seed=42)\n",
    "\n",
    "def generate_negative_samples(iterator: Iterator, negative_samples_per_sample: int):\n",
    "  for batched_sample in iterator:\n",
    "    codes_embeddings = batched_sample['code_embedding']\n",
    "    comments_embeddings = batched_sample['comment_embedding']\n",
    "    batch_indexes = range(len(codes_embeddings))\n",
    "\n",
    "    for index in batch_indexes:\n",
    "      indexes = [i for i in batch_indexes if i != index]\n",
    "      negative_indexes = random_generator.choice(indexes, negative_samples_per_sample, replace=False)\n",
    "\n",
    "      yield {\n",
    "        \"code_embedding\": codes_embeddings[index],\n",
    "        \"comment_embedding\": comments_embeddings[index],\n",
    "        \"target\": 1\n",
    "      }\n",
    "\n",
    "      for negative_index in negative_indexes:\n",
    "        yield {\n",
    "          \"code_embedding\": codes_embeddings[index],\n",
    "          \"comment_embedding\": comments_embeddings[negative_index],\n",
    "          \"target\": 0\n",
    "        }\n",
    "\n",
    "def with_neg_samples(dataset: Dataset, negative_samples_per_sample: int, batch_size = 100) -> Dataset:\n",
    "  assert negative_samples_per_sample <= batch_size, \"negative_samples_per_sample must not be greater than batch_size\"\n",
    "  if negative_samples_per_sample <= 0:\n",
    "    return dataset\n",
    "  \n",
    "  dataset_with_negative_samples: Dataset = Dataset.from_generator(lambda: generate_negative_samples(dataset.iter(batch_size=batch_size), negative_samples_per_sample)) # type: ignore\n",
    "  return dataset_with_negative_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embedding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "train_count = 2000\n",
    "train_dataset_path = f'../datasets/embeddings_python_train_{train_count}'\n",
    "train_pairs = load_from_cs_net(train_count)\n",
    "is_embeddings_dataset_stored = os.path.isdir(train_dataset_path)\n",
    "\n",
    "def generate_embeddings_in_batch(batched_sample):\n",
    "  codes = batched_sample['func_code_string']\n",
    "  comments = batched_sample['func_documentation_string']\n",
    "\n",
    "  return {\n",
    "    \"code_embedding\": code_model.encode(codes),\n",
    "    \"comment_embedding\": comment_model.encode(comments),\n",
    "  }\n",
    "\n",
    "embeddings_dataset: Dataset = Dataset.from_dict(load_from_disk(train_dataset_path)[:train_count]) if is_embeddings_dataset_stored else train_pairs.map(\n",
    "  generate_embeddings_in_batch, \n",
    "  batched=True, \n",
    "  batch_size=100,\n",
    "  remove_columns=list(train_pairs[0].keys()),\n",
    "  desc=\"Generating embeddings\"\n",
    ") # type: ignore\n",
    "\n",
    "if is_embeddings_dataset_stored == False:\n",
    "  embeddings_dataset.save_to_disk(train_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf_fit_dataset(negative_samples_per_sample: int):\n",
    "  tf_dataset = with_neg_samples(embeddings_dataset.shuffle(), negative_samples_per_sample).to_tf_dataset().map(lambda sample: ({\n",
    "    \"code_embedding\": sample[\"code_embedding\"],\n",
    "    \"comment_embedding\": sample[\"comment_embedding\"],\n",
    "  }, sample[\"target\"]))\n",
    "  dataset_size = tf_dataset.cardinality().numpy() # type: ignore\n",
    "  print(f\"training with {dataset_size} samples\")\n",
    "  validation_samples_count = int(dataset_size * 0.2)\n",
    "  \n",
    "  validation_ds = tf_dataset.take(validation_samples_count)\n",
    "  train_ds = tf_dataset.skip(validation_samples_count)\n",
    "\n",
    "  return (train_ds, validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "from models import build_dense_model\n",
    "\n",
    "\n",
    "neg_samples_count = [5]\n",
    "num_hidden_layers = 4\n",
    "for neg_count in neg_samples_count:\n",
    "  model = build_dense_model(num_hidden_layers=num_hidden_layers, input_shape=embedding_shape, model_name=f'dense_{num_hidden_layers}_neg_{neg_count}-dropout20')\n",
    "  train, validation = to_tf_fit_dataset(neg_count)\n",
    "  tensor_board_callback = callbacks.TensorBoard(log_dir=f'../logs/{model.name}')\n",
    "\n",
    "  model.fit(\n",
    "    train.batch(batch_size),\n",
    "    validation_data=validation.batch(batch_size),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch,\n",
    "    callbacks=[tensor_board_callback]\n",
    "  )\n",
    "  model.save(f'../models/{model.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, TypedDict, List\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "class SimilarityInput(TypedDict):\n",
    "  code_embedding: Any\n",
    "  comment_embedding: Any\n",
    "\n",
    "class SimilaryResult(TypedDict):\n",
    "  similarity: np.float32\n",
    "  pair_index: int\n",
    "\n",
    "class SearchResult(TypedDict):\n",
    "  similarity: np.float32\n",
    "  pair_index: int\n",
    "  query: str\n",
    "\n",
    "\n",
    "def get_similarities(inputs: List[SimilarityInput], model) -> List[SimilaryResult]:\n",
    "  tf_inputs = Dataset.from_list(inputs).to_tf_dataset(batch_size=10)\n",
    "  similarities = model.predict(tf_inputs, verbose=0).flatten()\n",
    "  results = [{ \"similarity\": similarity, \"pair_index\": index } for index, similarity in enumerate(similarities)]\n",
    "  return results # type: ignore\n",
    "\n",
    "\n",
    "def search(query, model) -> List[SearchResult]:\n",
    "  query_embedding = comment_model.encode([query]).flatten()\n",
    "  similarities = list(get_similarities(\n",
    "    inputs=[ { \"code_embedding\": embedding_pair['code_embedding'], \"comment_embedding\": query_embedding } for embedding_pair in embeddings_dataset],\n",
    "    model=model\n",
    "  ))\n",
    "\n",
    "  return [{\n",
    "    \"similarity\": similarity['similarity'],\n",
    "    \"pair_index\": similarity['pair_index'],\n",
    "    \"query\": query\n",
    "  } for similarity in similarities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentQuery(TypedDict):\n",
    "  comment_query: str\n",
    "  removed_word: str\n",
    "\n",
    "def generate_comment_queries(comment_tokens: List[str], max_words_to_remove = 30) -> Iterator[CommentQuery]:\n",
    "  words_to_remove_count = min(len(comment_tokens), max_words_to_remove)\n",
    "  word_indexes_to_remove = list(range(words_to_remove_count))\n",
    "  random_generator.shuffle(word_indexes_to_remove)\n",
    "\n",
    "  for word_index in word_indexes_to_remove:\n",
    "    comment_tokens_copy = comment_tokens.copy()\n",
    "    removed_word = comment_tokens_copy.pop(word_index)\n",
    "    comment_query = ' '.join(comment_tokens_copy)\n",
    "    yield {\n",
    "      \"comment_query\": comment_query,\n",
    "      \"removed_word\": removed_word\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import more_itertools\n",
    "\n",
    "def wrap(value: str, max_width = 20) -> str:\n",
    "  return \"<br>\".join([' '.join(sentence) for sentence in more_itertools.chunked(value.split(' '), n=max_width)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "class ExperimentLog:\n",
    "  def __init__(self, experiment_name: str) -> None:\n",
    "    self.experiment_name = experiment_name\n",
    "    self.experiment_results_dir = f'../results/{self.experiment_name}'\n",
    "    if not os.path.isdir(self.experiment_results_dir):\n",
    "      os.makedirs(self.experiment_results_dir)\n",
    "  \n",
    "  def save_figure(self, figure: go.Figure, figure_name: str):\n",
    "    image_extension = '.png'\n",
    "    figure_name_path = figure_name if figure_name.endswith(image_extension) else f'{figure_name}{image_extension}'\n",
    "    figure.write_image(os.path.join(self.experiment_results_dir, figure_name_path))\n",
    "\n",
    "  def save_file(self, data: str, file_name: str):\n",
    "    text_extension = '.txt'\n",
    "    file_name_path = file_name if file_name.endswith(text_extension) else f'{file_name}{text_extension}'\n",
    "    with open(os.path.join(self.experiment_results_dir, file_name_path), 'w') as file:\n",
    "      file.write(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generalization with a single comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generalization_experiment(search_model, samples_count):\n",
    "  for sample_index in range(samples_count):\n",
    "    comment_tokens: list = train_pairs[sample_index]['func_documentation_tokens']\n",
    "    experiment_results = []\n",
    "    \n",
    "    for index, comment_query in enumerate(generate_comment_queries(comment_tokens)) :\n",
    "      similarity = get_similarity(\n",
    "        query=comment_query['comment_query'],\n",
    "        code_embedding=embeddings_dataset[sample_index]['code_embedding'],\n",
    "        model=search_model,\n",
    "      )\n",
    "\n",
    "      experiment_results.append({\n",
    "        \"removed_word\": comment_query['removed_word'],\n",
    "        \"similarity\": similarity,\n",
    "        \"original_query\": ' '.join(comment_tokens),\n",
    "        \"index\": index,\n",
    "      })\n",
    "\n",
    "    yield experiment_results\n",
    "\n",
    "def get_similarity(query, code_embedding, model) -> np.float32:\n",
    "  query_embedding = comment_model.encode([query]).flatten()\n",
    "  predictions = get_similarities(\n",
    "    inputs=[{ \"code_embedding\": code_embedding, 'comment_embedding': query_embedding }], \n",
    "    model=model\n",
    "  )\n",
    "  return predictions[0]['similarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the experiment and plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_model = load_model('../models/dense_4_neg_5/')\n",
    "experiment_1_log = ExperimentLog('experiment_1_article')\n",
    "bad_sample_threshold = 0.2\n",
    "\n",
    "for index, results in enumerate(run_generalization_experiment(search_model=search_model, samples_count=100)):\n",
    "  sorted_results = sorted(results, key=lambda x: x['similarity'], reverse=True)\n",
    "  n_samples = len(sorted_results)\n",
    "\n",
    "  data_frame = pd.DataFrame({\n",
    "    \"removed_word\": [result['removed_word'] for result in sorted_results],\n",
    "    \"similarity\": [result['similarity'] for result in sorted_results],\n",
    "  })\n",
    "\n",
    "  original_query = wrap(sorted_results[0]['original_query'], 18)\n",
    "  fig = px.bar(data_frame, x=data_frame.index, y=\"similarity\", text_auto=True, title=f\"<sup>Comment: {original_query}</sup>\")\n",
    "  fig.update_xaxes(title_text='Removed word', tickvals=data_frame.index, ticktext=data_frame[\"removed_word\"].tolist())\n",
    "  fig.update_yaxes(title_text='Similarity')\n",
    "  experiment_1_log.save_figure(fig, f'sample_{index}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generalization with comments + search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment2Result(TypedDict):\n",
    "  sample_index: int\n",
    "  search_match_index: int\n",
    "  original_query: str\n",
    "  removed_word: str\n",
    "\n",
    "def get_search_match_index(search_ranking: List[SearchResult], sample_index: int):\n",
    "  for ranking_index, search_result in enumerate(search_ranking):\n",
    "    if search_result['pair_index'] == sample_index:\n",
    "      return ranking_index\n",
    "  raise ValueError(\"Pair not found in search ranking\")\n",
    "\n",
    "\n",
    "def run_experiment_2(search_model, pairs_count: int) -> Iterator[List[Experiment2Result]]:\n",
    "  for sample_index in tqdm(range(pairs_count), total=pairs_count, desc=\"Running experiment 2\"):\n",
    "    experiment_results: List[Experiment2Result] = []\n",
    "    pair = train_pairs[sample_index]\n",
    "    original_query = ' '.join(pair[\"func_documentation_tokens\"])\n",
    "\n",
    "    for comment_query in generate_comment_queries(pair['func_documentation_tokens']):\n",
    "      ranking = search(query=comment_query, model=search_model)\n",
    "      similarity_rank = sorted(ranking, key=lambda it: float(it['similarity']), reverse=True)\n",
    "\n",
    "      experiment_results.append({\n",
    "        \"sample_index\": sample_index,\n",
    "        \"search_match_index\": get_search_match_index(similarity_rank, sample_index),\n",
    "        \"original_query\": original_query,\n",
    "        \"removed_word\": comment_query[\"removed_word\"]\n",
    "      })\n",
    "      \n",
    "    yield experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_rate(results: List[Experiment2Result], k: int) -> float:\n",
    "  results_count = len(results)\n",
    "\n",
    "  hits = sum([result['search_match_index'] < k for result in results])\n",
    "  success_rate_k = 0 if hits == 0 else hits / results_count\n",
    "  return success_rate_k\n",
    "\n",
    "def mean_reciprocal_rank(results: List[Experiment2Result]) -> float:\n",
    "  results_count = len(results)\n",
    "  reciprocal_ranks_sum = sum([1 / (result['search_match_index'] + 1) for result in results])\n",
    "  mrr = reciprocal_ranks_sum / results_count\n",
    "  return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_model = load_model('../models/dense_4_neg_5/')\n",
    "experiment_2_log = ExperimentLog(experiment_name='experiment_2')\n",
    "experiment_2_results = list(run_experiment_2(search_model=search_model, pairs_count=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_values = [1, 5, 10]\n",
    "k_results = []\n",
    "for k in k_values:\n",
    "  success_rates = [success_rate(results, k) for results in experiment_2_results]\n",
    "  success_rates = sorted(success_rates)\n",
    "  for sample_index, s_rate in enumerate(success_rates):\n",
    "    k_results.append({\n",
    "      \"Sample\": sample_index,\n",
    "      \"SuccessRate@k\": s_rate,\n",
    "      \"k\": k\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(k_results)\n",
    "fig = px.line(df, x=\"Sample\", y=\"SuccessRate@k\", color=\"k\")\n",
    "experiment_2_log.save_figure(fig, 'success-rates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mrrs = sorted([mean_reciprocal_rank(results) for results in experiment_2_results])\n",
    "\n",
    "mrr_results = []\n",
    "for sample_index, mrr in enumerate(mrrs):\n",
    "  mrr_results.append({\n",
    "    \"Sample\": sample_index,\n",
    "    \"MRR\": mrr,\n",
    "  })\n",
    "\n",
    "df = pd.DataFrame(mrr_results)\n",
    "fig = px.line(df, x=\"Sample\", y=\"MRR\")\n",
    "experiment_2_log.save_figure(fig, 'mrr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_rate_k_mean(k):\n",
    "  results_count = len(experiment_2_results)\n",
    "  return sum([success_rate(results, k) for results in experiment_2_results]) / results_count\n",
    "\n",
    "success_rate_k_mean(1), success_rate_k_mean(5), success_rate_k_mean(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_rate_edges(k):\n",
    "  success_rates = [success_rate(results, k) for results in experiment_2_results]\n",
    "  return {\n",
    "    \"min\": min(success_rates),\n",
    "    \"max\": max(success_rates),\n",
    "  }\n",
    "\n",
    "success_rate_edges(1), success_rate_edges(5), success_rate_edges(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr_mean():\n",
    "  results_count = len(experiment_2_results)\n",
    "  return sum([mean_reciprocal_rank(results) for results in experiment_2_results]) / results_count\n",
    "\n",
    "mrr_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "mrrs_rank = sorted([(index, mean_reciprocal_rank(results)) for index, results in enumerate(experiment_2_results)], key=lambda it: it[1])\n",
    "mrr_dict = { it[0]: it[1] for it in mrrs_rank }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrrs_rank[:top_n], mrrs_rank[-top_n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generalization with CodeSearchNet queries + search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splits = load_dataset(dataset_name, 'python', split=['train', 'test', 'validation']) # type: ignore\n",
    "python_full_dataset = concatenate_datasets(python_splits) # type: ignore\n",
    "splits_info = python_splits[0].info.splits # type: ignore\n",
    "python_full_dataset_count = sum([splits_info[key].num_examples for key in splits_info.keys()]) # type: ignore\n",
    "\n",
    "pairs_dataset_lookup = { sample['func_code_url']: index  for index, sample in tqdm(enumerate(python_full_dataset), desc=\"Generating dict lookup\", total=python_full_dataset_count) }\n",
    "def search_by_url(url: str) -> Optional[int]:\n",
    "  try:\n",
    "    return pairs_dataset_lookup[url]\n",
    "  except:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_samples_path = '../datasets/query_samples'\n",
    "\n",
    "def remove_duplicates(dataset: Dataset) -> Dataset:\n",
    "  pandas_dataset = dataset.to_pandas().drop_duplicates(subset=['Language', 'Query', 'GitHubUrl', 'Relevance'], ignore_index=True) # type: ignore\n",
    "  dedup_dataset = Dataset.from_pandas(pandas_dataset)\n",
    "  return dedup_dataset\n",
    "\n",
    "def remove_queries_without_code(dataset: Dataset) -> Dataset:\n",
    "  return dataset.filter(lambda sample: search_by_url(sample['GitHubUrl']) is not None, desc=\"Filtering queries with no corresponding code\")\n",
    "\n",
    "def pre_process_query_samples() -> Dataset:\n",
    "  cs_net_queries_dataset: Dataset = Dataset.from_csv('../datasets/code_search_net_queries.csv') # type: ignore\n",
    "  \n",
    "  return remove_queries_without_code(remove_duplicates(cs_net_queries_dataset))\n",
    "\n",
    "def get_query_samples() -> Dataset:\n",
    "  try:\n",
    "    return Dataset.load_from_disk(query_samples_path)\n",
    "  except:\n",
    "    query_samples = pre_process_query_samples()\n",
    "    query_samples.save_to_disk(query_samples_path)\n",
    "    return query_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_code_embeddings(samples) -> Dataset:\n",
    "  query_texts = [sample['Query'] for sample in samples]\n",
    "  query_codes = [python_full_dataset[search_by_url(sample['GitHubUrl'])]['func_code_string'] for sample in samples]\n",
    "  assert len(query_texts) == len(query_codes), \"query_texts and query_codes arrays doesn't have the same length\"\n",
    "\n",
    "  query_embeddings = comment_model.encode(query_texts)\n",
    "  code_embeddings = code_model.encode(query_codes)\n",
    "\n",
    "  validation_dataset = []\n",
    "  for query_embedding, code_embedding in zip(query_embeddings, code_embeddings):\n",
    "    validation_dataset.append({\n",
    "      \"code_embedding\": code_embedding,\n",
    "      \"comment_embedding\": query_embedding,\n",
    "    })\n",
    "\n",
    "  return Dataset.from_list(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def validate(model, samples):\n",
    "  validation_dataset = get_query_code_embeddings(samples).to_tf_dataset(batch_size=10)\n",
    "\n",
    "  return {\n",
    "    \"predictions\": model.predict(validation_dataset, verbose=0).flatten(),\n",
    "    \"targets\": [sample['Relevance'] for sample in samples]\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prediction_correct(prediction, target) -> bool:\n",
    "  if target in [0, 1]:\n",
    "    return prediction <= 0.5\n",
    "  \n",
    "  if target in [2, 3]:\n",
    "    return prediction > 0.5\n",
    "  \n",
    "  raise ValueError(f\"target should be in range of [0, 3]. Instead, it has value of {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_query_samples = [sample for sample in get_query_samples() if sample['Language'].lower() == 'python']\n",
    "validation_query_samples_count = len(validation_query_samples)\n",
    "\n",
    "model = load_model('../models/dense_4_neg_5')\n",
    "search_result = validate(model, validation_query_samples)\n",
    "\n",
    "hits = sum([is_prediction_correct(prediction, target) for prediction, target in zip(search_result['predictions'], search_result['targets'])])\n",
    "success_percentage = hits / validation_query_samples_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_3_log = ExperimentLog('experiment_3')\n",
    "experiment_3_log.save_file(f'{hits} correct predictions out of {validation_query_samples_count} samples - {success_percentage:.2%} success rate', \"cs_net_queries_result.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
